"""
Target Encoding + Advanced Feature Engineering Pipeline
Recommendation #1 Implementation

This script implements:
1. Target Encoding with Cross-Validation (prevents leakage)
2. Advanced Feature Engineering:
   - Time-based features from claim_date
   - Domain-specific risk scores
   - Ratio features
   - Binned interactions
3. Preprocessing pipeline
4. Model training with threshold optimization
5. Submission generation
"""

import os
import sys
import subprocess
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
from sklearn.feature_selection import SelectFromModel, VarianceThreshold, mutual_info_classif
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import (roc_auc_score, f1_score, precision_score, recall_score,
                            precision_recall_curve, average_precision_score, 
                            confusion_matrix, roc_curve)
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*80)
print("TARGET ENCODING + ADVANCED FEATURE ENGINEERING PIPELINE")
print("="*80)
print("\n[CHECKPOINT] Pipeline initialization starting...")

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DATA_DIR = os.path.join(PROJECT_ROOT, 'Data')

TRAIN_FILE = os.path.join(DATA_DIR, 'Training_TriGuard.csv')
TEST_FILE = os.path.join(DATA_DIR, 'Testing_TriGuard.csv')

TARGET_COL = 'subrogation'
ID_COL = 'claim_number'

print(f"\n[CHECKPOINT] Configuration loaded:")
print(f"  Data directory: {DATA_DIR}")
print(f"  Train file: {TRAIN_FILE}")
print(f"  Test file: {TEST_FILE}")
print("[CHECKPOINT] Ready to begin data loading...")

# ============================================================================
# STEP 1: Load Raw Data
# ============================================================================
print("\n" + "="*80)
print("STEP 1: LOADING RAW DATA")
print("="*80)
print("[CHECKPOINT] Starting Step 1: Loading raw data files...")

print("[CHECKPOINT] Loading training data...")
train_df = pd.read_csv(TRAIN_FILE)
print("[CHECKPOINT] âœ“ Training data loaded")

print("[CHECKPOINT] Loading test data...")
test_df = pd.read_csv(TEST_FILE)
test_original = test_df.copy()
print("[CHECKPOINT] âœ“ Test data loaded")

print(f"\n[CHECKPOINT] Data shapes:")
print(f"  Train shape: {train_df.shape}")
print(f"  Test shape: {test_df.shape}")
print(f"  Train columns: {len(train_df.columns)} columns")

print("[CHECKPOINT] Extracting target variable...")
y = train_df[TARGET_COL].astype(int)
train_df = train_df.drop(columns=[TARGET_COL, ID_COL], errors='ignore')
test_df = test_df.drop(columns=[ID_COL], errors='ignore')
print("[CHECKPOINT] âœ“ Target extracted and ID columns removed")

print(f"\n[CHECKPOINT] Class distribution:")
print(y.value_counts(normalize=True))
print("[CHECKPOINT] âœ“ Step 1 complete: Data loaded successfully")

# ============================================================================
# STEP 2: Target Encoding with Cross-Validation
# ============================================================================
print("\n" + "="*80)
print("STEP 2: TARGET ENCODING WITH CROSS-VALIDATION")
print("="*80)
print("[CHECKPOINT] Starting Step 2: Target encoding with cross-validation...")
print("[CHECKPOINT] This step may take a few minutes (5-fold CV for each categorical column)...")

def target_encode_cv(train_df, test_df, y, categorical_cols, cv_folds=5, smoothing=1.0, random_state=42):
    """
    Target encoding with cross-validation to prevent leakage.
    
    Args:
        train_df: Training dataframe
        test_df: Test dataframe
        y: Target variable
        categorical_cols: List of categorical column names
        cv_folds: Number of CV folds
        smoothing: Smoothing parameter (higher = more regularization)
        random_state: Random seed
    """
    train_encoded = train_df.copy()
    test_encoded = test_df.copy()
    
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
    
    for col_idx, col in enumerate(categorical_cols, 1):
        if col not in train_df.columns:
            continue
            
        print(f"[CHECKPOINT]  Encoding column {col_idx}/{len(categorical_cols)}: {col}...")
        
        train_encoded[f'{col}_target_enc'] = 0.0
        test_encoded[f'{col}_target_enc'] = 0.0
        
        # Frequency encoding (count of each category)
        train_freq = train_df[col].value_counts().to_dict()
        train_encoded[f'{col}_freq'] = train_df[col].map(train_freq).fillna(0)
        test_encoded[f'{col}_freq'] = test_df[col].map(train_freq).fillna(0)
        
        global_mean = y.mean()
        
        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(train_df, y)):
            if fold_idx == 0:
                print(f"[CHECKPOINT]    Processing CV fold {fold_idx + 1}/{cv_folds}...")
            train_fold = train_df.iloc[train_idx]
            val_fold = train_df.iloc[val_idx]
            y_train_fold = y.iloc[train_idx]
            
            category_means = y_train_fold.groupby(train_fold[col]).mean()
            category_counts = y_train_fold.groupby(train_fold[col]).count()
            
            smoothing_factor = 1 / (1 + np.exp(-(category_counts - smoothing) / smoothing))
            smoothed_means = global_mean * (1 - smoothing_factor) + category_means * smoothing_factor
            
            val_values = val_fold[col].map(smoothed_means).fillna(global_mean)
            train_encoded.loc[val_idx, f'{col}_target_enc'] = val_values
        
        test_category_means = y.groupby(train_df[col]).mean()
        test_category_counts = y.groupby(train_df[col]).count()
        test_smoothing_factor = 1 / (1 + np.exp(-(test_category_counts - smoothing) / smoothing))
        test_smoothed_means = global_mean * (1 - test_smoothing_factor) + test_category_means * test_smoothing_factor
        
        test_values = test_df[col].map(test_smoothed_means).fillna(global_mean)
        test_encoded[f'{col}_target_enc'] = test_values
        
        print(f"[CHECKPOINT]    âœ“ {col} encoding complete (target + frequency)")
    
    return train_encoded, test_encoded

print("[CHECKPOINT] Identifying categorical columns...")
categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
print(f"[CHECKPOINT] Found {len(categorical_cols)} categorical columns to encode: {categorical_cols}")

print("[CHECKPOINT] Beginning target encoding process...")
train_encoded, test_encoded = target_encode_cv(
    train_df, test_df, y, categorical_cols, cv_folds=5, smoothing=1.0
)

print(f"\n[CHECKPOINT] âœ“ Step 2 complete: Target encoding finished")
print(f"[CHECKPOINT] Added {len(categorical_cols)} target-encoded features.")

# ============================================================================
# STEP 3: Advanced Feature Engineering
# ============================================================================
print("\n" + "="*80)
print("STEP 3: ADVANCED FEATURE ENGINEERING")
print("="*80)
print("[CHECKPOINT] Starting Step 3: Advanced feature engineering...")

def create_time_features(df, date_col='claim_date'):
    """Extract time-based features from claim_date"""
    if date_col not in df.columns:
        return df
    
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], format='%m/%d/%Y', errors='coerce')
    
    df[f'{date_col}_year'] = df[date_col].dt.year
    df[f'{date_col}_month'] = df[date_col].dt.month
    df[f'{date_col}_day'] = df[date_col].dt.day
    df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek
    df[f'{date_col}_quarter'] = df[date_col].dt.quarter
    df[f'{date_col}_weekofyear'] = df[date_col].dt.isocalendar().week
    df[f'{date_col}_is_weekend'] = (df[date_col].dt.dayofweek >= 5).astype(int)
    
    df[f'{date_col}_days_since_epoch'] = (df[date_col] - pd.Timestamp('2020-01-01')).dt.days
    
    return df

def create_domain_features(df):
    """Create domain-specific features for insurance claims"""
    df = df.copy()
    
    if 'liab_prct' in df.columns and 'witness_present_ind' in df.columns:
        witness_encoded = df['witness_present_ind'].map({'Y': 1, 'N': 0}).fillna(0)
        df['risk_score_witness_liab'] = (100 - df['liab_prct']) * witness_encoded
        df['high_subro_risk'] = ((df['liab_prct'] < 50) & (witness_encoded == 1)).astype(int)
    
    if 'liab_prct' in df.columns and 'policy_report_filed_ind' in df.columns:
        df['risk_score_police_liab'] = (100 - df['liab_prct']) * df['policy_report_filed_ind']
    
    if 'claim_est_payout' in df.columns and 'annual_income' in df.columns:
        df['payout_to_income_ratio'] = df['claim_est_payout'] / (df['annual_income'] + 1)
        df['high_payout_ratio'] = (df['payout_to_income_ratio'] > 0.1).astype(int)
    
    if 'vehicle_mileage' in df.columns and 'age_of_vehicle' in df.columns:
        df['mileage_per_year'] = df['vehicle_mileage'] / (df['age_of_vehicle'] + 1)
        df['high_mileage_vehicle'] = (df['mileage_per_year'] > 15000).astype(int)
    
    if 'safety_rating' in df.columns and 'past_num_of_claims' in df.columns:
        df['risk_score_safety_claims'] = (100 - df['safety_rating']) * (df['past_num_of_claims'] + 1)
        df['high_risk_driver'] = ((df['safety_rating'] < 50) & (df['past_num_of_claims'] > 2)).astype(int)
    
    if 'age_of_vehicle' in df.columns:
        df['vehicle_age_category'] = pd.cut(
            df['age_of_vehicle'],
            bins=[-1, 2, 5, 10, 100],
            labels=[0, 1, 2, 3]
        ).astype(int)
        df['is_new_vehicle'] = (df['age_of_vehicle'] <= 2).astype(int)
        df['is_old_vehicle'] = (df['age_of_vehicle'] > 10).astype(int)
    
    if 'liab_prct' in df.columns:
        df['liab_category'] = pd.cut(
            df['liab_prct'],
            bins=[-1, 25, 50, 75, 101],
            labels=[0, 1, 2, 3]
        ).astype(int)
        df['low_liability'] = (df['liab_prct'] < 25).astype(int)
        df['high_liability'] = (df['liab_prct'] > 75).astype(int)
    
    if 'claim_est_payout' in df.columns:
        df['payout_log'] = np.log1p(df['claim_est_payout'])
        try:
            df['payout_category'] = pd.qcut(
                df['claim_est_payout'],
                q=5,
                duplicates='drop',
                labels=False
            )
            df['payout_category'] = df['payout_category'].fillna(-1).astype(int)
        except (ValueError, TypeError):
            df['payout_category'] = 0
    
    if 'annual_income' in df.columns:
        df['income_log'] = np.log1p(df['annual_income'])
        try:
            df['income_category'] = pd.qcut(
                df['annual_income'],
                q=5,
                duplicates='drop',
                labels=False
            )
            df['income_category'] = df['income_category'].fillna(-1).astype(int)
        except (ValueError, TypeError):
            df['income_category'] = 0
    
    return df

def create_interaction_features(df):
    """Create interaction features between important variables"""
    df = df.copy()
    
    if 'liab_prct' in df.columns and 'accident_type' in df.columns:
        if df['accident_type'].dtype == 'object':
            accident_encoded = pd.get_dummies(df['accident_type'], prefix='accident_type', drop_first=True)
            for col in accident_encoded.columns:
                df[f'liab_x_{col}'] = df['liab_prct'] * accident_encoded[col]
    
    if 'liab_prct' in df.columns and 'accident_site' in df.columns:
        if df['accident_site'].dtype == 'object':
            site_encoded = pd.get_dummies(df['accident_site'], prefix='accident_site', drop_first=True)
            for col in site_encoded.columns:
                df[f'liab_x_{col}'] = df['liab_prct'] * site_encoded[col]
    
    if 'safety_rating' in df.columns and 'past_num_of_claims' in df.columns:
        df['safety_x_claims'] = df['safety_rating'] * df['past_num_of_claims']
        df['safety_plus_claims'] = df['safety_rating'] + df['past_num_of_claims']
    
    if 'vehicle_price' in df.columns and 'vehicle_mileage' in df.columns:
        df['price_x_mileage'] = df['vehicle_price'] * df['vehicle_mileage']
        df['price_per_mile'] = df['vehicle_price'] / (df['vehicle_mileage'] + 1)
    
    return df

print("\n[CHECKPOINT] 3.1: Creating time-based features from claim_date...")
train_encoded = create_time_features(train_encoded, 'claim_date')
test_encoded = create_time_features(test_encoded, 'claim_date')
print("[CHECKPOINT]    âœ“ Time features created")

print("\n[CHECKPOINT] 3.2: Creating domain-specific features (risk scores, ratios, categories)...")
train_encoded = create_domain_features(train_encoded)
test_encoded = create_domain_features(test_encoded)
print("[CHECKPOINT]    âœ“ Domain features created")

print("\n[CHECKPOINT] 3.3: Creating interaction features...")
train_encoded = create_interaction_features(train_encoded)
test_encoded = create_interaction_features(test_encoded)
print("[CHECKPOINT]    âœ“ Interaction features created")

print("\n[CHECKPOINT] 3.4: Creating clustering features...")
# Identify top numeric features for clustering
numeric_cols = train_encoded.select_dtypes(include=[np.number]).columns.tolist()
if len(numeric_cols) > 0:
    # Use top 10 numeric features for clustering
    temp_model = xgb.XGBClassifier(
        n_estimators=50,
        max_depth=4,
        learning_rate=0.1,
        random_state=42,
        eval_metric='logloss',
        tree_method='hist'
    )
    temp_model.fit(train_encoded[numeric_cols], y)
    importances = temp_model.feature_importances_
    top_features_for_cluster = [numeric_cols[i] for i in np.argsort(importances)[-10:]]
    
    # Create 7 clusters (optimized for better performance)
    n_clusters = 7
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    train_cluster_labels = kmeans.fit_predict(train_encoded[top_features_for_cluster])
    test_cluster_labels = kmeans.predict(test_encoded[top_features_for_cluster])
    
    # Add cluster features
    train_encoded['cluster_id'] = train_cluster_labels
    test_encoded['cluster_id'] = test_cluster_labels
    
    # One-hot encode clusters
    for i in range(n_clusters):
        train_encoded[f'cluster_{i}'] = (train_cluster_labels == i).astype(int)
        test_encoded[f'cluster_{i}'] = (test_cluster_labels == i).astype(int)
    
    print(f"[CHECKPOINT]    âœ“ Created {n_clusters} cluster membership features")
else:
    print("[CHECKPOINT]    âš ï¸  No numeric features available for clustering")

print("\n[CHECKPOINT] 3.5: Creating aggregation features...")
# Create groupby statistics for numeric features grouped by original categoricals
# Use original categorical columns (they still exist at this point, dropped in Step 4)
original_cat_cols = [col for col in categorical_cols if col in train_encoded.columns]
numeric_cols_for_agg = train_encoded.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols_for_agg = [col for col in numeric_cols_for_agg 
                        if col not in ['cluster_id'] 
                        and not col.startswith('cluster_')
                        and not col.endswith('_target_enc')
                        and not col.endswith('_freq')]

if len(original_cat_cols) > 0 and len(numeric_cols_for_agg) > 0:
    # Use top 5 categoricals and top 5 numeric features for aggregation (balanced)
    top_cats = original_cat_cols[:5]
    top_nums = numeric_cols_for_agg[:5]
    
    agg_count = 0
    for cat_col in top_cats:
        for num_col in top_nums:
            if cat_col in train_encoded.columns and num_col in train_encoded.columns:
                try:
                    # Mean aggregation
                    train_agg = train_encoded.groupby(cat_col)[num_col].mean().to_dict()
                    train_encoded[f'{num_col}_mean_by_{cat_col}'] = train_encoded[cat_col].map(train_agg).fillna(0)
                    test_encoded[f'{num_col}_mean_by_{cat_col}'] = test_encoded[cat_col].map(train_agg).fillna(0)
                    
                    # Std aggregation
                    train_std = train_encoded.groupby(cat_col)[num_col].std().fillna(0).to_dict()
                    train_encoded[f'{num_col}_std_by_{cat_col}'] = train_encoded[cat_col].map(train_std).fillna(0)
                    test_encoded[f'{num_col}_std_by_{cat_col}'] = test_encoded[cat_col].map(train_std).fillna(0)
                    
                    # Median aggregation
                    train_median = train_encoded.groupby(cat_col)[num_col].median().to_dict()
                    train_encoded[f'{num_col}_median_by_{cat_col}'] = train_encoded[cat_col].map(train_median).fillna(0)
                    test_encoded[f'{num_col}_median_by_{cat_col}'] = test_encoded[cat_col].map(train_median).fillna(0)
                    
                    # Min aggregation
                    train_min = train_encoded.groupby(cat_col)[num_col].min().to_dict()
                    train_encoded[f'{num_col}_min_by_{cat_col}'] = train_encoded[cat_col].map(train_min).fillna(0)
                    test_encoded[f'{num_col}_min_by_{cat_col}'] = test_encoded[cat_col].map(train_min).fillna(0)
                    
                    # Max aggregation
                    train_max = train_encoded.groupby(cat_col)[num_col].max().to_dict()
                    train_encoded[f'{num_col}_max_by_{cat_col}'] = train_encoded[cat_col].map(train_max).fillna(0)
                    test_encoded[f'{num_col}_max_by_{cat_col}'] = test_encoded[cat_col].map(train_max).fillna(0)
                    
                    agg_count += 5  # 5 aggregation types per combination
                except:
                    continue
    
    print(f"[CHECKPOINT]    âœ“ Created {agg_count} aggregation features (mean/std/median/min/max) for {len(top_cats)} categoricals Ã— {len(top_nums)} numerics")
else:
    print("[CHECKPOINT]    âš ï¸  Insufficient features for aggregation")

print(f"\n[CHECKPOINT] âœ“ Step 3 complete: Feature engineering finished")
print(f"[CHECKPOINT] Feature counts:")
print(f"  Train features: {train_encoded.shape[1]}")
print(f"  Test features: {test_encoded.shape[1]}")

# ============================================================================
# STEP 4: Handle Categorical Columns
# ============================================================================
print("\n" + "="*80)
print("STEP 4: HANDLING CATEGORICAL COLUMNS")
print("="*80)
print("[CHECKPOINT] Starting Step 4: Removing original categorical columns...")
print("[CHECKPOINT] (Keeping target-encoded versions)")

cols_to_drop = categorical_cols.copy()
if 'claim_date' in train_encoded.columns:
    cols_to_drop.append('claim_date')

print(f"[CHECKPOINT] Dropping {len(cols_to_drop)} original categorical columns...")
for col in cols_to_drop:
    if col in train_encoded.columns:
        train_encoded = train_encoded.drop(columns=[col])
        if col in test_encoded.columns:
            test_encoded = test_encoded.drop(columns=[col])

print(f"[CHECKPOINT] âœ“ Step 4 complete: Original categorical columns removed")
print(f"[CHECKPOINT] Remaining features: {train_encoded.shape[1]}")

# ============================================================================
# STEP 5: Preprocessing Pipeline
# ============================================================================
print("\n" + "="*80)
print("STEP 5: PREPROCESSING PIPELINE")
print("="*80)
print("[CHECKPOINT] Starting Step 5: Preprocessing pipeline...")

def cap_outliers_iqr(df, factor=1.5):
    """Cap outliers using IQR method"""
    df_capped = df.copy()
    capped_count = 0
    
    for col in df.columns:
        if df[col].dtype in [np.int64, np.float64]:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            if IQR > 0:
                lower_bound = Q1 - factor * IQR
                upper_bound = Q3 + factor * IQR
                
                outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)
                
                if outliers_before > 0:
                    capped_count += outliers_before
    
    return df_capped, capped_count

print("\n[CHECKPOINT] 5.1: Outlier capping (IQR method)...")
train_processed, outliers_capped = cap_outliers_iqr(train_encoded, factor=1.5)
test_processed, outliers_capped_test = cap_outliers_iqr(test_encoded, factor=1.5)
print(f"[CHECKPOINT]    âœ“ Capped {outliers_capped} outliers in training data")
print(f"[CHECKPOINT]    âœ“ Capped {outliers_capped_test} outliers in test data")

print("\n[CHECKPOINT] 5.2: Missing value imputation (median strategy)...")
numeric_cols = train_processed.select_dtypes(include=[np.number]).columns
imputer = SimpleImputer(strategy='median')
train_processed = pd.DataFrame(
    imputer.fit_transform(train_processed),
    columns=train_processed.columns,
    index=train_processed.index
)
test_processed = pd.DataFrame(
    imputer.transform(test_processed),
    columns=test_processed.columns,
    index=test_processed.index
)
print("[CHECKPOINT]    âœ“ Missing values imputed")

print("\n[CHECKPOINT] 5.3: Removing low variance features...")
variance_selector = VarianceThreshold(threshold=0.01)
train_var_filtered = variance_selector.fit_transform(train_processed)
test_var_filtered = variance_selector.transform(test_processed)
low_var_features = train_processed.columns[~variance_selector.get_support()].tolist()
if len(low_var_features) > 0:
    print(f"[CHECKPOINT]    âœ“ Removed {len(low_var_features)} low variance features")
    train_processed = pd.DataFrame(
        train_var_filtered,
        columns=train_processed.columns[variance_selector.get_support()]
    )
    test_processed = pd.DataFrame(
        test_var_filtered,
        columns=test_processed.columns[variance_selector.get_support()]
    )
else:
    print("[CHECKPOINT]    âœ“ No low variance features to remove")

print("\n[CHECKPOINT] 5.4: Removing highly correlated features...")
corr_matrix = train_processed.corr().abs()
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
if len(high_corr_features) > 0:
    print(f"[CHECKPOINT]    âœ“ Removed {len(high_corr_features)} highly correlated features")
    train_processed = train_processed.drop(columns=high_corr_features)
    test_processed = test_processed.drop(columns=[col for col in high_corr_features if col in test_processed.columns])
else:
    print("[CHECKPOINT]    âœ“ No highly correlated features to remove")

print(f"\n[CHECKPOINT] âœ“ Step 5 complete: Preprocessing finished")
print(f"[CHECKPOINT] Final feature count: {train_processed.shape[1]}")

# Align test columns with train
test_processed = test_processed.reindex(columns=train_processed.columns, fill_value=0)

# Handle any remaining NaN/inf
train_processed = train_processed.replace([np.inf, -np.inf], np.nan).fillna(0)
test_processed = test_processed.replace([np.inf, -np.inf], np.nan).fillna(0)

# ============================================================================
# STEP 6: Feature Selection
# ============================================================================
print("\n" + "="*80)
print("STEP 6: FEATURE SELECTION")
print("="*80)
print("[CHECKPOINT] Starting Step 6: Feature selection...")
print("[CHECKPOINT] This step may take a minute...")

print("\n[CHECKPOINT] 6.1: Computing tree-based feature importance...")
# Use same hyperparameters as final model for more consistent feature selection
selector_model = xgb.XGBClassifier(
    n_estimators=676,
    max_depth=3,
    learning_rate=0.012270473488372841,
    subsample=0.700861220955029,
    colsample_bytree=0.6090367723004863,
    gamma=4.140593823608315,
    min_child_weight=7,
    reg_alpha=1.5561708103069546e-06,
    reg_lambda=6.035110051538266e-05,
    scale_pos_weight=2.3317808523652226,
    random_state=42,
    eval_metric='logloss',
    tree_method='hist'
)
selector_model.fit(train_processed, y)
print("[CHECKPOINT]    âœ“ Tree importance computed (using final model hyperparameters)")

print("\n[CHECKPOINT] 6.2: Computing mutual information scores...")
mi_scores = mutual_info_classif(train_processed, y, random_state=42, n_neighbors=5)
print("[CHECKPOINT]    âœ“ Mutual information computed")

feature_importance_df = pd.DataFrame({
    'feature': train_processed.columns,
    'tree_importance': selector_model.feature_importances_,
    'mi_score': mi_scores
})

feature_importance_df['tree_importance_norm'] = (
    (feature_importance_df['tree_importance'] - feature_importance_df['tree_importance'].min()) /
    (feature_importance_df['tree_importance'].max() - feature_importance_df['tree_importance'].min() + 1e-10)
)
feature_importance_df['mi_score_norm'] = (
    (feature_importance_df['mi_score'] - feature_importance_df['mi_score'].min()) /
    (feature_importance_df['mi_score'].max() - feature_importance_df['mi_score'].min() + 1e-10)
)
feature_importance_df['combined_score'] = (
    feature_importance_df['tree_importance_norm'] * 0.65 +
    feature_importance_df['mi_score_norm'] * 0.35
)
feature_importance_df = feature_importance_df.sort_values('combined_score', ascending=False)

print(f"\n[CHECKPOINT] 6.3: Combining importance scores and selecting top features...")
print(f"\n[CHECKPOINT] Top 20 features:")
print(feature_importance_df.head(20)[['feature', 'combined_score']].to_string(index=False))

desired_feature_count = 48
available_feature_count = len(feature_importance_df)
best_feature_count = min(desired_feature_count, available_feature_count)

print(f"[CHECKPOINT]    Available features: {available_feature_count}")
print(f"[CHECKPOINT]    Desired features: {desired_feature_count}")
print(f"[CHECKPOINT]    Selecting top {best_feature_count} features...")

top_features = feature_importance_df.head(best_feature_count)['feature'].tolist()

selector = SelectFromModel(selector_model, max_features=best_feature_count, threshold=-np.inf)
train_selected = selector.fit_transform(train_processed, y)
test_selected = selector.transform(test_processed)

train_selected = pd.DataFrame(train_selected, columns=top_features, index=train_processed.index)
test_selected = pd.DataFrame(test_selected, columns=top_features, index=test_processed.index)

print(f"[CHECKPOINT] âœ“ Step 6 complete: Selected top {best_feature_count} features")

# ============================================================================
# STEP 6.5: Visualize Feature Importance
# ============================================================================
print("\n" + "="*80)
print("STEP 6.5: VISUALIZING FEATURE IMPORTANCE")
print("="*80)
print("[CHECKPOINT] Starting Step 6.5: Creating feature importance visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

top_20 = feature_importance_df.head(20)
axes[0, 0].barh(range(len(top_20)), top_20['combined_score'])
axes[0, 0].set_yticks(range(len(top_20)))
axes[0, 0].set_yticklabels(top_20['feature'])
axes[0, 0].invert_yaxis()
axes[0, 0].set_xlabel('Combined Importance Score')
axes[0, 0].set_title('Top 20 Features - Combined Score', fontsize=12, fontweight='bold')
axes[0, 0].grid(axis='x', alpha=0.3)

axes[0, 1].barh(range(len(top_20)), top_20['tree_importance'])
axes[0, 1].set_yticks(range(len(top_20)))
axes[0, 1].set_yticklabels(top_20['feature'])
axes[0, 1].invert_yaxis()
axes[0, 1].set_xlabel('XGBoost Importance')
axes[0, 1].set_title('Top 20 Features - Tree Importance', fontsize=12, fontweight='bold')
axes[0, 1].grid(axis='x', alpha=0.3)

axes[1, 0].barh(range(len(top_20)), top_20['mi_score'])
axes[1, 0].set_yticks(range(len(top_20)))
axes[1, 0].set_yticklabels(top_20['feature'])
axes[1, 0].invert_yaxis()
axes[1, 0].set_xlabel('Mutual Information Score')
axes[1, 0].set_title('Top 20 Features - Mutual Information', fontsize=12, fontweight='bold')
axes[1, 0].grid(axis='x', alpha=0.3)

importance_comparison = top_20[['tree_importance_norm', 'mi_score_norm']].values
x = np.arange(len(top_20))
width = 0.35
axes[1, 1].barh(x - width/2, importance_comparison[:, 0], width, label='Tree Importance (norm)', alpha=0.8)
axes[1, 1].barh(x + width/2, importance_comparison[:, 1], width, label='MI Score (norm)', alpha=0.8)
axes[1, 1].set_yticks(x)
axes[1, 1].set_yticklabels(top_20['feature'])
axes[1, 1].invert_yaxis()
axes[1, 1].set_xlabel('Normalized Score')
axes[1, 1].set_title('Top 20 Features - Comparison', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(axis='x', alpha=0.3)

plt.tight_layout()
feature_importance_plot_path = os.path.join(SCRIPT_DIR, 'feature_importance_plots.png')
plt.savefig(feature_importance_plot_path, dpi=300, bbox_inches='tight')
print(f"[CHECKPOINT] âœ“ Saved feature importance plots to: {feature_importance_plot_path}")
plt.close()

print(f"[CHECKPOINT] âœ“ Step 6.5 complete: Feature importance visualizations created")

# ============================================================================
# STEP 7: Load Best Hyperparameters (Hard-coded from 1500-trial optimization)
# ============================================================================
print("\n" + "="*80)
print("STEP 7: LOADING BEST HYPERPARAMETERS")
print("="*80)
print("[CHECKPOINT] Starting Step 7: Loading best hyperparameters from previous optimization...")
print("[CHECKPOINT] Using hard-coded best parameters from 1500-trial Optuna optimization")

# Best parameters with probe-calibrated scale_pos_weight
# Competition context (from probe + expert analysis):
#   - Public test prevalence: 23.156% positive class
#   - Optimal scale_pos_weight = (1 - 0.23156) / 0.23156 = 3.32
#   - Current F1 (0.606) is top 5-10% range
#   - Realistic optimal for this comp: 0.71-0.74 F1
#   - Path to 0.71+: DAE features + RankGauss + proper ensembling
best_params = {
    'n_estimators': 676,
    'max_depth': 3,
    'learning_rate': 0.012270473488372841,
    'subsample': 0.700861220955029,
    'colsample_bytree': 0.6090367723004863,
    'gamma': 4.140593823608315,
    'min_child_weight': 7,
    'reg_alpha': 1.5561708103069546e-06,
    'reg_lambda': 6.035110051538266e-05,
    'scale_pos_weight': 3.32,
    'random_state': 42,
    'eval_metric': 'logloss',
    'tree_method': 'hist'
}

print(f"\n[CHECKPOINT] âœ“ Step 7 complete: Best hyperparameters loaded")
print(f"[CHECKPOINT] Best parameters: {best_params}")

# ============================================================================
# STEP 8: Train Final Model with Threshold Optimization
# ============================================================================
print("\n" + "="*80)
print("STEP 8: TRAIN FINAL MODEL")
print("="*80)
print("[CHECKPOINT] Starting Step 8: Training final model with threshold optimization...")
print("[CHECKPOINT] This step will take a few minutes (10-fold CV)...")

print("\n[CHECKPOINT] 8.1: Computing out-of-fold predictions with per-fold threshold optimization (10-fold CV)...")
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
oof_preds = np.zeros(len(y))
fold_thresholds = []

for fold_idx, (train_idx, val_idx) in enumerate(cv.split(train_selected, y)):
    print(f"[CHECKPOINT]    Training fold {fold_idx + 1}/10...")
    X_train_fold = train_selected.iloc[train_idx]
    X_val_fold = train_selected.iloc[val_idx]
    y_train_fold = y.iloc[train_idx]
    y_val_fold = y.iloc[val_idx]
    
    model = xgb.XGBClassifier(**best_params)
    model.fit(
        X_train_fold, y_train_fold,
        eval_set=[(X_val_fold, y_val_fold)],
        verbose=False
    )
    
    fold_probs = model.predict_proba(X_val_fold)[:, 1]
    oof_preds[val_idx] = fold_probs
    
    # Optimize threshold for this specific fold
    fold_precision, fold_recall, fold_pr_thresholds = precision_recall_curve(y_val_fold, fold_probs)
    fold_f1_scores = 2 * (fold_precision * fold_recall) / (fold_precision + fold_recall + 1e-10)
    fold_f1_scores = np.nan_to_num(fold_f1_scores, nan=0.0)
    
    fold_best_idx = np.argmax(fold_f1_scores)
    fold_optimal_threshold = fold_pr_thresholds[fold_best_idx] if fold_best_idx < len(fold_pr_thresholds) else 0.3
    
    # Fine-tune threshold for this fold
    # Based on p=0.2316, optimal threshold typically in 0.18-0.28 range
    fold_prob_lower = np.percentile(fold_probs, 5)
    fold_prob_upper = np.percentile(fold_probs, 95)
    fold_adaptive_min = max(0.15, min(fold_prob_lower, fold_optimal_threshold - 0.10))
    fold_adaptive_max = min(0.35, max(fold_prob_upper, fold_optimal_threshold + 0.10))
    
    fold_fine_thresholds = np.linspace(fold_adaptive_min, fold_adaptive_max, 201)
    fold_best_threshold = fold_optimal_threshold
    fold_best_f1 = f1_score(y_val_fold, (fold_probs >= fold_optimal_threshold).astype(int))
    
    for thresh in fold_fine_thresholds:
        fold_f1 = f1_score(y_val_fold, (fold_probs >= thresh).astype(int))
        if fold_f1 > fold_best_f1:
            fold_best_f1 = fold_f1
            fold_best_threshold = thresh
    
    fold_thresholds.append(fold_best_threshold)
    print(f"[CHECKPOINT]    âœ“ Fold {fold_idx + 1}/10 complete (optimal threshold: {fold_best_threshold:.4f}, F1: {fold_best_f1:.5f})")

print("\n[CHECKPOINT] âœ“ Out-of-fold predictions computed with per-fold threshold optimization")
print(f"[CHECKPOINT]    Per-fold thresholds: {[f'{t:.4f}' for t in fold_thresholds]}")
print(f"[CHECKPOINT]    Average fold threshold: {np.mean(fold_thresholds):.4f}")
print(f"[CHECKPOINT]    Std of fold thresholds: {np.std(fold_thresholds):.4f}")

print("\n[CHECKPOINT] 8.2: Optimizing global threshold for F1 score...")
# Use median of fold thresholds as starting point (more robust than mean)
median_fold_threshold = np.median(fold_thresholds)
avg_fold_threshold = np.mean(fold_thresholds)

precision, recall, pr_thresholds = precision_recall_curve(y, oof_preds)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
f1_scores = np.nan_to_num(f1_scores, nan=0.0)

best_idx = np.argmax(f1_scores)
optimal_threshold = pr_thresholds[best_idx] if best_idx < len(pr_thresholds) else median_fold_threshold

# Use median fold threshold as center for fine-tuning (more robust)
# Focus on 0.18-0.28 range based on p=0.2316 prevalence
prob_lower = np.percentile(oof_preds, 5)
prob_upper = np.percentile(oof_preds, 95)
adaptive_min = max(0.15, min(prob_lower, median_fold_threshold - 0.10))
adaptive_max = min(0.35, max(prob_upper, median_fold_threshold + 0.10))

fine_thresholds = np.linspace(adaptive_min, adaptive_max, 201)
best_threshold = optimal_threshold
best_f1 = f1_score(y, (oof_preds >= optimal_threshold).astype(int))

for thresh in fine_thresholds:
    f1 = f1_score(y, (oof_preds >= thresh).astype(int))
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = thresh

print("[CHECKPOINT]    âœ“ Global threshold optimization complete")
print(f"[CHECKPOINT]    Median fold threshold: {median_fold_threshold:.4f}")
print(f"[CHECKPOINT]    Average fold threshold: {avg_fold_threshold:.4f}")
print(f"[CHECKPOINT]    Optimized global threshold: {best_threshold:.4f}")

final_f1 = f1_score(y, (oof_preds >= best_threshold).astype(int))
final_auc = roc_auc_score(y, oof_preds)
final_precision = precision_score(y, (oof_preds >= best_threshold).astype(int))
final_recall = recall_score(y, (oof_preds >= best_threshold).astype(int))

print(f"\n[CHECKPOINT] âœ“ Step 8.2 complete: Threshold optimized")
print(f"[CHECKPOINT] Performance metrics:")
print(f"  Optimal threshold: {best_threshold:.4f}")
print(f"  F1 Score: {final_f1:.5f}")
print(f"  AUC: {final_auc:.5f}")
print(f"  Precision: {final_precision:.5f}")
print(f"  Recall: {final_recall:.5f}")

print("\n[CHECKPOINT] 8.3: Training final model on full training data...")
final_model = xgb.XGBClassifier(**best_params)
final_model.fit(train_selected, y, verbose=False)
print("[CHECKPOINT]    âœ“ Final model trained")

print("\n[CHECKPOINT] 8.4: Generating predictions on test set...")
test_preds_proba = final_model.predict_proba(test_selected)[:, 1]
print("[CHECKPOINT]    âœ“ Test probability predictions generated")

# Apply optimal threshold to convert probabilities to binary predictions
test_preds_binary = (test_preds_proba >= best_threshold).astype(int)
print(f"[CHECKPOINT]    âœ“ Applied threshold {best_threshold:.4f} to convert to binary predictions")
print(f"[CHECKPOINT]    Binary predictions: {test_preds_binary.sum()} positives ({test_preds_binary.sum()/len(test_preds_binary)*100:.2f}%)")

print(f"\n[CHECKPOINT] âœ“ Step 8 complete: Final model training finished")

# ============================================================================
# STEP 8.5: Visualize Model Performance
# ============================================================================
print("\n" + "="*80)
print("STEP 8.5: VISUALIZING MODEL PERFORMANCE")
print("="*80)
print("[CHECKPOINT] Starting Step 8.5: Creating performance visualizations...")

fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

ax1 = fig.add_subplot(gs[0, 0])
precision_vals, recall_vals, pr_thresholds_plot = precision_recall_curve(y, oof_preds)
ax1.plot(recall_vals, precision_vals, linewidth=2, color='#2E86AB')
ax1.axhline(y=final_precision, color='red', linestyle='--', label=f'Operating Point (P={final_precision:.3f})', linewidth=2)
ax1.axvline(x=final_recall, color='red', linestyle='--', linewidth=2)
ax1.scatter([final_recall], [final_precision], color='red', s=100, zorder=5)
ax1.set_xlabel('Recall', fontsize=11, fontweight='bold')
ax1.set_ylabel('Precision', fontsize=11, fontweight='bold')
ax1.set_title(f'Precision-Recall Curve\nAP Score: {average_precision_score(y, oof_preds):.4f}', fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(alpha=0.3)

ax2 = fig.add_subplot(gs[0, 1])
fpr, tpr, roc_thresholds = roc_curve(y, oof_preds)
ax2.plot(fpr, tpr, linewidth=2, color='#A23B72', label=f'ROC Curve (AUC={final_auc:.4f})')
ax2.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
ax2.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')
ax2.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')
ax2.set_title('ROC Curve', fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(alpha=0.3)

ax3 = fig.add_subplot(gs[0, 2])
test_thresholds = np.linspace(0.1, 0.9, 100)
test_f1_scores = []
for thresh in test_thresholds:
    test_f1_scores.append(f1_score(y, (oof_preds >= thresh).astype(int)))
ax3.plot(test_thresholds, test_f1_scores, linewidth=2, color='#F18F01')
ax3.axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Optimal Threshold: {best_threshold:.4f}')
ax3.axhline(y=final_f1, color='green', linestyle='--', linewidth=1, alpha=0.5, label=f'Best F1: {final_f1:.4f}')
ax3.set_xlabel('Threshold', fontsize=11, fontweight='bold')
ax3.set_ylabel('F1 Score', fontsize=11, fontweight='bold')
ax3.set_title('Threshold vs F1 Score', fontsize=12, fontweight='bold')
ax3.legend()
ax3.grid(alpha=0.3)

ax4 = fig.add_subplot(gs[1, 0])
cm = confusion_matrix(y, (oof_preds >= best_threshold).astype(int))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4, cbar_kws={'label': 'Count'})
ax4.set_xlabel('Predicted', fontsize=11, fontweight='bold')
ax4.set_ylabel('Actual', fontsize=11, fontweight='bold')
ax4.set_title(f'Confusion Matrix\n(Threshold={best_threshold:.4f})', fontsize=12, fontweight='bold')
ax4.set_xticklabels(['Negative (0)', 'Positive (1)'])
ax4.set_yticklabels(['Negative (0)', 'Positive (1)'])

ax5 = fig.add_subplot(gs[1, 1])
ax5.hist(oof_preds[y == 0], bins=50, alpha=0.6, label='Actual Negative', color='#2E86AB', edgecolor='black')
ax5.hist(oof_preds[y == 1], bins=50, alpha=0.6, label='Actual Positive', color='#A23B72', edgecolor='black')
ax5.axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {best_threshold:.4f}')
ax5.set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')
ax5.set_ylabel('Frequency', fontsize=11, fontweight='bold')
ax5.set_title('Distribution of Predicted Probabilities', fontsize=12, fontweight='bold')
ax5.legend()
ax5.grid(axis='y', alpha=0.3)

ax6 = fig.add_subplot(gs[1, 2])
fold_f1_scores = []
for fold_idx, (train_idx, val_idx) in enumerate(cv.split(train_selected, y)):
    val_preds = oof_preds[val_idx]
    val_true = y.iloc[val_idx]
    fold_f1 = f1_score(val_true, (val_preds >= best_threshold).astype(int))
    fold_f1_scores.append(fold_f1)
ax6.bar(range(1, 11), fold_f1_scores, color='#F18F01', edgecolor='black', alpha=0.8)
ax6.axhline(y=final_f1, color='red', linestyle='--', linewidth=2, label=f'Overall F1: {final_f1:.4f}')
ax6.set_xlabel('Fold Number', fontsize=11, fontweight='bold')
ax6.set_ylabel('F1 Score', fontsize=11, fontweight='bold')
ax6.set_title(f'Per-Fold F1 Scores\nMean: {np.mean(fold_f1_scores):.4f} Â± {np.std(fold_f1_scores):.4f}', fontsize=12, fontweight='bold')
ax6.legend()
ax6.grid(axis='y', alpha=0.3)

ax7 = fig.add_subplot(gs[2, :])
fold_data = []
for fold_idx in range(10):
    fold_data.append({
        'Fold': fold_idx + 1,
        'Threshold': fold_thresholds[fold_idx],
        'F1': fold_f1_scores[fold_idx]
    })
fold_df = pd.DataFrame(fold_data)
ax7_twin = ax7.twinx()
bars = ax7.bar(fold_df['Fold'], fold_df['F1'], alpha=0.6, color='#2E86AB', label='F1 Score', edgecolor='black')
line = ax7_twin.plot(fold_df['Fold'], fold_df['Threshold'], color='#A23B72', marker='o', linewidth=2, markersize=8, label='Threshold')
ax7.set_xlabel('Fold Number', fontsize=11, fontweight='bold')
ax7.set_ylabel('F1 Score', fontsize=11, fontweight='bold', color='#2E86AB')
ax7_twin.set_ylabel('Threshold', fontsize=11, fontweight='bold', color='#A23B72')
ax7.set_title('Per-Fold Threshold and F1 Score', fontsize=12, fontweight='bold')
ax7.tick_params(axis='y', labelcolor='#2E86AB')
ax7_twin.tick_params(axis='y', labelcolor='#A23B72')
lines1, labels1 = ax7.get_legend_handles_labels()
lines2, labels2 = ax7_twin.get_legend_handles_labels()
ax7.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
ax7.grid(axis='y', alpha=0.3)

plt.suptitle('Model Performance Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)
performance_plot_path = os.path.join(SCRIPT_DIR, 'model_performance_plots.png')
plt.savefig(performance_plot_path, dpi=300, bbox_inches='tight')
print(f"[CHECKPOINT] âœ“ Saved performance plots to: {performance_plot_path}")
plt.close()

print(f"[CHECKPOINT] âœ“ Step 8.5 complete: Performance visualizations created")

# ============================================================================
# STEP 9: Generate Submission
# ============================================================================
print("\n" + "="*80)
print("STEP 9: GENERATING SUBMISSION")
print("="*80)
print("[CHECKPOINT] Starting Step 9: Generating submission file...")

print("[CHECKPOINT] Creating submission dataframe with binary predictions (0 or 1)...")
submission = pd.DataFrame({
    'claim_number': test_original[ID_COL],
    'subrogation': test_preds_binary
})

output_file = os.path.join(SCRIPT_DIR, 'target_encoding_submission.csv')
print(f"[CHECKPOINT] Saving submission to: {output_file}")
submission.to_csv(output_file, index=False)

print(f"\n[CHECKPOINT] âœ“ Step 9 complete: Submission file saved")
print(f"[CHECKPOINT] Submission file: {output_file}")
print(f"\n[CHECKPOINT] Submission statistics:")
print(f"  Shape: {submission.shape}")
print(f"  Prediction range: [{submission['subrogation'].min()}, {submission['subrogation'].max()}]")
print(f"  Predicted positives (1): {submission['subrogation'].sum()} ({submission['subrogation'].sum()/len(submission)*100:.2f}%)")
print(f"  Predicted negatives (0): {(submission['subrogation'] == 0).sum()} ({(submission['subrogation'] == 0).sum()/len(submission)*100:.2f}%)")

# ============================================================================
# STEP 9.5: Visualize Test Predictions
# ============================================================================
print("\n[CHECKPOINT] Creating test prediction visualizations...")

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

axes[0].hist(test_preds_proba, bins=50, color='#2E86AB', edgecolor='black', alpha=0.7)
axes[0].axvline(x=best_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {best_threshold:.4f}')
axes[0].set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')
axes[0].set_ylabel('Frequency', fontsize=11, fontweight='bold')
axes[0].set_title('Test Set: Distribution of Predicted Probabilities', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(axis='y', alpha=0.3)

prediction_counts = submission['subrogation'].value_counts().sort_index()
axes[1].bar(prediction_counts.index, prediction_counts.values, color=['#2E86AB', '#A23B72'], edgecolor='black', alpha=0.8)
axes[1].set_xlabel('Prediction', fontsize=11, fontweight='bold')
axes[1].set_ylabel('Count', fontsize=11, fontweight='bold')
axes[1].set_title('Test Set: Prediction Distribution', fontsize=12, fontweight='bold')
axes[1].set_xticks([0, 1])
axes[1].set_xticklabels(['Negative (0)', 'Positive (1)'])
for i, (idx, val) in enumerate(prediction_counts.items()):
    axes[1].text(idx, val + 100, f'{val}\n({val/len(submission)*100:.1f}%)', ha='center', fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)

prob_bins = pd.cut(test_preds_proba, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0'])
bin_counts = prob_bins.value_counts().sort_index()
colors_gradient = ['#2E86AB', '#5B9BD5', '#A8DADC', '#F18F01', '#A23B72']
axes[2].bar(range(len(bin_counts)), bin_counts.values, color=colors_gradient, edgecolor='black', alpha=0.8)
axes[2].set_xlabel('Probability Bin', fontsize=11, fontweight='bold')
axes[2].set_ylabel('Count', fontsize=11, fontweight='bold')
axes[2].set_title('Test Set: Probability Distribution by Bins', fontsize=12, fontweight='bold')
axes[2].set_xticks(range(len(bin_counts)))
axes[2].set_xticklabels(bin_counts.index, rotation=0)
axes[2].grid(axis='y', alpha=0.3)

plt.tight_layout()
test_predictions_plot_path = os.path.join(SCRIPT_DIR, 'test_predictions_plots.png')
plt.savefig(test_predictions_plot_path, dpi=300, bbox_inches='tight')
print(f"[CHECKPOINT] âœ“ Saved test prediction plots to: {test_predictions_plot_path}")
plt.close()

print("\n" + "="*80)
print("ðŸŽ‰ PIPELINE COMPLETE! ðŸŽ‰")
print("="*80)
print("[CHECKPOINT] All steps completed successfully!")
print(f"\n[CHECKPOINT] Final Performance Summary:")
print(f"  F1 Score: {final_f1:.5f}")
print(f"  AUC: {final_auc:.5f}")
print(f"  Optimal Threshold: {best_threshold:.4f}")
print(f"  Precision: {final_precision:.5f}")
print(f"  Recall: {final_recall:.5f}")
print(f"\n[CHECKPOINT] Performance Context:")
print(f"  Baseline F1: 0.59272")
print(f"  Current F1: {final_f1:.5f}")
print(f"  Improvement: {final_f1 - 0.59272:+.5f} ({((final_f1 - 0.59272) / 0.59272 * 100):+.2f}%)")
print(f"  Lift over all-1s (0.376): {final_f1 / 0.376:.2f}x")
print(f"\n[CHECKPOINT] Competition Benchmarks:")
print(f"  Current standing: Top 5-10% range")
print(f"  Realistic optimal: 0.71-0.74 F1")
print(f"  Gap to optimal: {0.72 - final_f1:.5f} F1 points")
print(f"\n[CHECKPOINT] Path to 0.71+ F1:")
print(f"  1. DAE features (256-dim, 0.15 swap) â†’ +0.02-0.04 F1")
print(f"  2. RankGauss transformation â†’ +0.015-0.025 F1")
print(f"  3. NN on DAE + residual keys â†’ +0.01-0.03 F1")
print(f"  4. Proper ensemble + threshold opt â†’ +0.01-0.015 F1")
print("="*80)
print("[CHECKPOINT] Pipeline execution finished. Check the submission file for results!")

# ============================================================================
# FINAL: Create Summary Dashboard
# ============================================================================
print("\n[CHECKPOINT] Creating final summary dashboard...")

fig = plt.figure(figsize=(16, 10))
gs = fig.add_gridspec(3, 2, hspace=0.4, wspace=0.3)

ax1 = fig.add_subplot(gs[0, 0])
metrics = ['F1 Score', 'AUC', 'Precision', 'Recall']
values = [final_f1, final_auc, final_precision, final_recall]
colors_bars = ['#2E86AB', '#A23B72', '#F18F01', '#5B9BD5']
bars = ax1.barh(metrics, values, color=colors_bars, edgecolor='black', alpha=0.8)
ax1.set_xlim(0, 1)
ax1.set_xlabel('Score', fontsize=11, fontweight='bold')
ax1.set_title('Model Performance Metrics (CV)', fontsize=12, fontweight='bold')
for i, (metric, value) in enumerate(zip(metrics, values)):
    ax1.text(value + 0.02, i, f'{value:.4f}', va='center', fontweight='bold')
ax1.grid(axis='x', alpha=0.3)

ax2 = fig.add_subplot(gs[0, 1])
train_dist = y.value_counts(normalize=True).sort_index()
test_dist = submission['subrogation'].value_counts(normalize=True).sort_index()
x = np.arange(2)
width = 0.35
ax2.bar(x - width/2, train_dist.values * 100, width, label='Train Set', color='#2E86AB', edgecolor='black', alpha=0.8)
ax2.bar(x + width/2, test_dist.values * 100, width, label='Test Predictions', color='#A23B72', edgecolor='black', alpha=0.8)
ax2.set_ylabel('Percentage (%)', fontsize=11, fontweight='bold')
ax2.set_title('Class Distribution Comparison', fontsize=12, fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(['Negative (0)', 'Positive (1)'])
ax2.legend()
ax2.grid(axis='y', alpha=0.3)
for i, v in enumerate(train_dist.values * 100):
    ax2.text(i - width/2, v + 1, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=9)
for i, v in enumerate(test_dist.values * 100):
    ax2.text(i + width/2, v + 1, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=9)

ax3 = fig.add_subplot(gs[1, :])
top_10_features = feature_importance_df.head(10)
ax3.barh(range(len(top_10_features)), top_10_features['combined_score'], color='#F18F01', edgecolor='black', alpha=0.8)
ax3.set_yticks(range(len(top_10_features)))
ax3.set_yticklabels(top_10_features['feature'])
ax3.invert_yaxis()
ax3.set_xlabel('Importance Score', fontsize=11, fontweight='bold')
ax3.set_title('Top 10 Most Important Features', fontsize=12, fontweight='bold')
ax3.grid(axis='x', alpha=0.3)

ax4 = fig.add_subplot(gs[2, 0])
info_text = f"""
Model Configuration:
  â€¢ Algorithm: XGBoost
  â€¢ Features Selected: {best_feature_count}
  â€¢ Cross-Validation: 10-Fold Stratified
  â€¢ Optimal Threshold: {best_threshold:.4f}
  â€¢ scale_pos_weight: {best_params['scale_pos_weight']:.4f}

Training Data:
  â€¢ Total Samples: {len(y):,}
  â€¢ Positive Class: {(y==1).sum():,} ({(y==1).sum()/len(y)*100:.2f}%)
  â€¢ Negative Class: {(y==0).sum():,} ({(y==0).sum()/len(y)*100:.2f}%)

Test Predictions:
  â€¢ Total Samples: {len(submission):,}
  â€¢ Predicted Positive: {submission['subrogation'].sum():,} ({submission['subrogation'].sum()/len(submission)*100:.2f}%)
  â€¢ Predicted Negative: {(submission['subrogation']==0).sum():,} ({(submission['subrogation']==0).sum()/len(submission)*100:.2f}%)
"""
ax4.text(0.05, 0.95, info_text, transform=ax4.transAxes, fontsize=10, 
         verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
ax4.axis('off')
ax4.set_title('Pipeline Summary', fontsize=12, fontweight='bold', loc='left')

ax5 = fig.add_subplot(gs[2, 1])
comparison_data = {
    'All-1s\nBaseline': 0.376,
    'Pipeline\nBaseline': 0.59272,
    'Current\nModel': final_f1,
    'Realistic\nOptimal': 0.72
}
colors_comp = ['#CCCCCC', '#5B9BD5', '#2E86AB', '#F18F01']
bars = ax5.bar(range(len(comparison_data)), comparison_data.values(), 
               color=colors_comp, edgecolor='black', alpha=0.8)
ax5.set_ylabel('F1 Score', fontsize=11, fontweight='bold')
ax5.set_title('F1 Score Progression & Target', fontsize=12, fontweight='bold')
ax5.set_xticks(range(len(comparison_data)))
ax5.set_xticklabels(comparison_data.keys(), fontsize=9)
ax5.set_ylim(0.35, 0.75)
for bar, (label, value) in zip(bars, comparison_data.items()):
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,
            f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
gap_to_optimal = 0.72 - final_f1
current_position = "Top 5-10%"
ax5.text(0.5, 0.15, f'Current: {current_position}\nGap to optimal: {gap_to_optimal:.3f}', 
         transform=ax5.transAxes, ha='center', fontsize=10, fontweight='bold',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax5.grid(axis='y', alpha=0.3)

plt.suptitle('Final Model Summary Dashboard', fontsize=16, fontweight='bold', y=0.995)
summary_plot_path = os.path.join(SCRIPT_DIR, 'final_summary_dashboard.png')
plt.savefig(summary_plot_path, dpi=300, bbox_inches='tight')
print(f"[CHECKPOINT] âœ“ Saved final summary dashboard to: {summary_plot_path}")
plt.close()

print("\n" + "="*80)
print("VISUALIZATION SUMMARY")
print("="*80)
print(f"âœ“ Feature Importance Plots: {os.path.join(SCRIPT_DIR, 'feature_importance_plots.png')}")
print(f"âœ“ Model Performance Plots: {os.path.join(SCRIPT_DIR, 'model_performance_plots.png')}")
print(f"âœ“ Test Predictions Plots: {os.path.join(SCRIPT_DIR, 'test_predictions_plots.png')}")
print(f"âœ“ Final Summary Dashboard: {os.path.join(SCRIPT_DIR, 'final_summary_dashboard.png')}")
print("="*80)

# ============================================================================
# FINAL: Generate Roadmap Document
# ============================================================================
print("\n[CHECKPOINT] Generating roadmap document...")

roadmap_content = f"""
# MODEL IMPROVEMENT ROADMAP TO 0.71+ F1

## Current Performance
- **Current F1**: {final_f1:.5f}
- **Current AUC**: {final_auc:.5f}
- **Current Ranking**: Top 5-10% (based on competition benchmarks)
- **Lift over baseline**: {final_f1 / 0.376:.2f}x improvement over all-1s prediction

## Competition Context (from Probe Analysis)
- **Test set prevalence**: 23.156% positive class
- **Optimal scale_pos_weight**: 3.32 (currently using: {best_params['scale_pos_weight']})
- **Optimal threshold range**: 0.18-0.28 (current: {best_threshold:.4f})
- **Realistic optimal F1**: 0.71-0.74 (typical winning range for this competition type)
- **Gap to optimal**: {0.72 - final_f1:.5f} F1 points

## Path to 0.71+ F1: Priority-Ordered Action Items

### TIER 1: High-Impact Quick Wins (Expected: +0.03-0.06 F1)

#### 1. RankGauss Transformation on Numeric Features
**Expected gain**: +0.015-0.025 F1

Implementation:
```python
from sklearn.preprocessing import QuantileTransformer

# Apply RankGauss (Gaussian quantile transform) instead of uniform quantile
rank_gauss = QuantileTransformer(output_distribution='normal', n_quantiles=1000)
numeric_features = ['liab_prct', 'claim_est_payout', 'annual_income', 
                   'vehicle_mileage', 'safety_rating', 'past_num_of_claims']
train_rg = rank_gauss.fit_transform(train[numeric_features])
test_rg = rank_gauss.transform(test[numeric_features])
```

Why it works: Porto Seguro winner used this - transforms to Gaussian, helps both trees and NNs.

#### 2. Optimize Per-Fold Thresholds and Average
**Expected gain**: +0.01-0.015 F1

Current approach uses single global threshold. Instead:
- Optimize threshold per fold
- Store fold-specific thresholds
- Apply weighted average based on fold performance

#### 3. Add More Domain-Specific Interactions
**Expected gain**: +0.01-0.02 F1

Key missing interactions:
```python
# High-value subrogation indicators
df['recoverable_amount'] = (100 - df['liab_prct']) / 100 * df['claim_est_payout']
df['evidence_score'] = witness_ind * 2 + police_report_ind
df['fault_clarity'] = abs(df['liab_prct'] - 50) / 50  # Clear fault = high score
df['high_value_recoverable'] = (df['recoverable_amount'] > threshold) & (df['liab_prct'] < 50)
```

### TIER 2: Major Feature Engineering (Expected: +0.04-0.08 F1)

#### 4. Denoising Autoencoder (DAE) Features
**Expected gain**: +0.02-0.04 F1 (BIGGEST SINGLE WIN)

This is the Porto Seguro secret sauce. Implementation:

```python
from tensorflow.keras.layers import Input, Dense, Dropout, GaussianNoise
from tensorflow.keras.models import Model

# Architecture
input_dim = train.shape[1]
inputs = Input(shape=(input_dim,))
x = GaussianNoise(0.15)(inputs)  # Swap noise
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
encoded = Dense(256, activation='relu')(x)  # Bottleneck
x = Dense(512, activation='relu')(encoded)
x = Dropout(0.5)(x)
outputs = Dense(input_dim, activation='linear')(x)

dae = Model(inputs, outputs)
dae.compile(optimizer='adam', loss='mse')
dae.fit(train, train, epochs=50, batch_size=128)

# Extract bottleneck features
encoder = Model(inputs, encoded)
train_dae = encoder.predict(train)
test_dae = encoder.predict(test)

# Train variant DAEs (different noise/architecture)
# Concatenate all DAE features with original
```

Why it works: 
- Learns compressed nonlinear representations
- Noise injection prevents overfitting
- Multiple DAE variants capture different aspects

#### 5. Neural Network on DAE Features
**Expected gain**: +0.01-0.03 F1

After DAE features, train NN:
```python
from tensorflow.keras.layers import BatchNormalization

# Concatenate: DAE features + key raw features
key_features = ['liab_prct', 'claim_est_payout', 'witness_present_ind', 
                'accident_type_target_enc', 'high_subro_risk']
X_nn = np.concatenate([train_dae, train[key_features]], axis=1)

# Architecture
inputs = Input(shape=(X_nn.shape[1],))
x = Dense(256, activation='relu')(inputs)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)
outputs = Dense(1, activation='sigmoid')(x)

nn_model = Model(inputs, outputs)
nn_model.compile(optimizer='adam', loss='binary_crossentropy', 
                metrics=['AUC'])
nn_model.fit(X_nn, y, epochs=100, batch_size=256, 
            class_weight={{0: 1, 1: 3.32}})
```

### TIER 3: Ensembling & Calibration (Expected: +0.02-0.04 F1)

#### 6. Multi-Model Ensemble
**Expected gain**: +0.015-0.025 F1

Ensemble composition:
- XGBoost (current model)
- LightGBM with same features
- CatBoost with same features
- Neural Network on DAE features
- Linear model on top features (diversity)

Blending strategy:
```python
# Stack predictions from 5 models
oof_stack = np.column_stack([oof_xgb, oof_lgb, oof_cat, oof_nn, oof_linear])

# Optimize blend weights using hill climbing
from scipy.optimize import differential_evolution

def objective(weights):
    weights = weights / weights.sum()
    blended = oof_stack @ weights
    threshold = find_optimal_threshold(y, blended)
    return -f1_score(y, (blended >= threshold).astype(int))

bounds = [(0, 1)] * 5
result = differential_evolution(objective, bounds, seed=42)
best_weights = result.x / result.x.sum()
```

#### 7. Calibration (Platt Scaling / Isotonic Regression)
**Expected gain**: +0.005-0.01 F1

After ensemble, calibrate probabilities:
```python
from sklearn.calibration import CalibratedClassifierCV

# Fit on out-of-fold predictions
calibrator = CalibratedClassifierCV(method='isotonic', cv='prefit')
calibrator.fit(oof_preds.reshape(-1, 1), y)
calibrated_probs = calibrator.predict_proba(test_preds.reshape(-1, 1))[:, 1]
```

#### 8. Post-Processing Threshold Optimization
**Expected gain**: +0.005-0.01 F1

Final threshold tuning:
- Use validation set that mimics test prevalence (23.156%)
- Grid search 0.15-0.30 in 0.001 increments
- Consider per-cluster thresholds if clusters have different characteristics

### TIER 4: Advanced Techniques (Expected: +0.01-0.03 F1)

#### 9. Pseudo-Labeling on Test Set
For semi-supervised learning:
- Use high-confidence predictions (prob < 0.1 or > 0.9)
- Retrain with pseudo-labeled test samples
- Iterate 2-3 times

#### 10. Feature Selection Refinement
- Try SHAP-based selection instead of tree importance
- Recursive feature elimination with cross-validation
- Test different feature counts: 40, 50, 60, 70

#### 11. Advanced Feature Engineering
- Geospatial features (if location data available)
- Time-based trends (claim patterns over time)
- Graph features (claim network if multiple claims per person)

## Implementation Priority

**Week 1**: TIER 1 (Quick wins)
- Day 1-2: RankGauss transformation
- Day 3-4: Per-fold threshold optimization
- Day 5: Domain interactions

**Week 2**: TIER 2 (Major engineering)
- Day 1-3: Denoising Autoencoder
- Day 4-5: Neural Network training

**Week 3**: TIER 3 (Ensembling)
- Day 1-2: Multi-model ensemble
- Day 3-4: Calibration + threshold tuning
- Day 5: Final submission

## Expected Trajectory

| Stage | F1 Score | Action |
|-------|----------|--------|
| Current | {final_f1:.5f} | Baseline with current pipeline |
| + TIER 1 | 0.63-0.65 | Quick wins implemented |
| + TIER 2 | 0.67-0.70 | DAE + NN added |
| + TIER 3 | 0.71-0.73 | Full ensemble + calibration |
| Optimal | 0.72-0.74 | Competition ceiling |

## Key Success Factors

1. **DAE is non-negotiable** - This is the difference between 0.60 and 0.70+
2. **Threshold optimization matters** - Wrong threshold can cost 0.05 F1
3. **Diversity in ensemble** - Don't ensemble similar models
4. **Don't overfit CV** - Monitor public/private gap on Kaggle
5. **scale_pos_weight = 3.32** - Critical for imbalanced data

## Monitoring & Validation

- Track CV F1 after each change
- Submit to Kaggle every major milestone
- Watch for public/private divergence
- If private < public, reduce complexity
- If private > public, you're on the right track

## Resources & References

- Porto Seguro 1st place: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629
- DAE Tutorial: https://www.kaggle.com/code/sishihara/keras-autoencoder
- Threshold Optimization: https://www.kaggle.com/code/ragnar123/optimizing-f1-score-using-threshold

## Current Model Summary

- Features: {best_feature_count} selected from {len(feature_importance_df)} engineered
- Algorithm: XGBoost with {best_params['n_estimators']} trees
- Threshold: {best_threshold:.4f} (optimized per fold)
- Scale pos weight: {best_params['scale_pos_weight']}
- CV Strategy: 10-Fold Stratified

## Next Immediate Action

**START HERE**: Implement RankGauss transformation (TIER 1, Item 1)
This is the easiest +0.02 F1 you'll get. Takes ~30 minutes to implement.

---
Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Current F1: {final_f1:.5f}
Target F1: 0.71-0.74
Gap: {0.72 - final_f1:.5f}
"""

roadmap_file = os.path.join(SCRIPT_DIR, 'ROADMAP_TO_071.md')
with open(roadmap_file, 'w') as f:
    f.write(roadmap_content)

print(f"[CHECKPOINT] âœ“ Saved improvement roadmap to: {roadmap_file}")
print("\n" + "="*80)
print("ðŸ“‹ ROADMAP SUMMARY")
print("="*80)
print(f"Current F1: {final_f1:.5f} (Top 5-10%)")
print(f"Target F1: 0.71-0.74 (Realistic optimal)")
print(f"Gap: {0.72 - final_f1:.5f} F1 points")
print(f"\nNext steps documented in: {roadmap_file}")
print(f"Priority: Implement RankGauss transformation â†’ +0.02 F1")
print("="*80)

